# ç¬¬ä¹ç« ï¼šé›†æˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ”¹å˜æˆ‘ä»¬ä¸è®¡ç®—æœºäº¤äº’çš„æ–¹å¼ã€‚åœ¨å‰ç«¯åº”ç”¨ä¸­é›†æˆ LLMï¼Œå¯ä»¥åˆ›é€ æ™ºèƒ½å¯¹è¯ã€å†…å®¹ç”Ÿæˆã€ä»£ç è¾…åŠ©ç­‰å…¨æ–°ä½“éªŒã€‚æœ¬ç« å°†æ¢ç´¢å¦‚ä½•åœ¨å‰ç«¯åº”ç”¨ä¸­é›†æˆ LLMï¼Œæ„å»ºæ™ºèƒ½åŒ–çš„ç”¨æˆ·ä½“éªŒã€‚

## LLM åœ¨å‰ç«¯çš„åº”ç”¨åœºæ™¯

```mermaid
graph TB
    A[LLM å‰ç«¯åº”ç”¨] --> B[æ™ºèƒ½å¯¹è¯]
    A --> C[å†…å®¹ç”Ÿæˆ]
    A --> D[ä»£ç è¾…åŠ©]
    A --> E[æ•°æ®åˆ†æ]
    
    B --> B1[å®¢æœåŠ©æ‰‹]
    B --> B2[å­¦ä¹ ä¼™ä¼´]
    
    C --> C1[æ–‡æ¡ˆç”Ÿæˆ]
    C --> C2[å†…å®¹æ‘˜è¦]
    
    D --> D1[ä»£ç è§£é‡Š]
    D --> D2[ä»£ç ç”Ÿæˆ]
    
    E --> E1[æ•°æ®æ´å¯Ÿ]
    E --> E2[æŠ¥å‘Šç”Ÿæˆ]
    
    style A fill:#90EE90
```

**åº”ç”¨åœºæ™¯æ•°æ®**ï¼ˆåŸºäºå¸‚åœºè°ƒç ”ï¼‰ï¼š

| åœºæ™¯ | ä½¿ç”¨ç‡ | ç”¨æˆ·æ»¡æ„åº¦ | ä»·å€¼è¯„åˆ† |
|------|--------|-----------|----------|
| **æ™ºèƒ½å¯¹è¯** | 85% | 4.2/5 | â­â­â­â­â­ |
| **å†…å®¹ç”Ÿæˆ** | 70% | 4.0/5 | â­â­â­â­ |
| **ä»£ç è¾…åŠ©** | 60% | 4.5/5 | â­â­â­â­â­ |
| **æ•°æ®åˆ†æ** | 45% | 3.8/5 | â­â­â­â­ |

---

## 9.1 å‰ç«¯è°ƒç”¨ LLM API çš„æ¨¡å¼ä¸æœ€ä½³å®è·µ

åœ¨å‰ç«¯åº”ç”¨ä¸­è°ƒç”¨ LLM APIï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„æ¨¡å¼å’Œå¤„ç†å„ç§è¾¹ç•Œæƒ…å†µã€‚è¿™ä¸€èŠ‚å°†ä»‹ç»æœ€ä½³å®è·µã€‚

### API è°ƒç”¨æ¨¡å¼å¯¹æ¯”

```mermaid
graph LR
    A[LLM API è°ƒç”¨] --> B[ç›´æ¥è°ƒç”¨]
    A --> C[ä»£ç†è°ƒç”¨]
    A --> D[è¾¹ç¼˜å‡½æ•°]
    
    B --> B1[ç®€å•]
    B --> B2[æš´éœ²å¯†é’¥]
    
    C --> C1[å®‰å…¨]
    C --> C2[éœ€è¦åç«¯]
    
    D --> D1[æ€§èƒ½å¥½]
    D --> D2[æˆæœ¬ä½]
    
    style C fill:#90EE90
    style D fill:#90EE90
```

### æ¨¡å¼ä¸€ï¼šç›´æ¥è°ƒç”¨ï¼ˆä¸æ¨èï¼‰

**å®ç°**ï¼š
```typescript
// âŒ ä¸æ¨èï¼šç›´æ¥åœ¨å‰ç«¯è°ƒç”¨ï¼Œæš´éœ² API å¯†é’¥
const response = await fetch('https://api.openai.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${API_KEY}`, // å¯†é’¥æš´éœ²ï¼
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Hello' }],
  }),
});
```

**é—®é¢˜**ï¼š
- âŒ API å¯†é’¥æš´éœ²åœ¨å‰ç«¯ä»£ç ä¸­
- âŒ æ— æ³•æ§åˆ¶ä½¿ç”¨é‡å’Œæˆæœ¬
- âŒ å®¹æ˜“è¢«æ»¥ç”¨

### æ¨¡å¼äºŒï¼šåç«¯ä»£ç†ï¼ˆæ¨èï¼‰

**æ¶æ„**ï¼š
```mermaid
graph LR
    A[å‰ç«¯] --> B[åç«¯ API]
    B --> C[LLM API]
    B --> D[è®¤è¯/é™æµ]
    B --> E[æ—¥å¿—/ç›‘æ§]
    
    style B fill:#90EE90
```

**å‰ç«¯å®ç°**ï¼š
```typescript
// âœ… æ¨èï¼šé€šè¿‡åç«¯ä»£ç†è°ƒç”¨
async function callLLM(messages: Message[]) {
  const response = await fetch('/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      // ä½¿ç”¨ç”¨æˆ·è®¤è¯ tokenï¼Œè€Œä¸æ˜¯ API å¯†é’¥
      'Authorization': `Bearer ${userToken}`,
    },
    body: JSON.stringify({ messages }),
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status}`);
  }

  return await response.json();
}
```

**åç«¯å®ç°**ï¼ˆNode.js ç¤ºä¾‹ï¼‰ï¼š
```typescript
// åç«¯ API è·¯ç”±
import express from 'express';
import OpenAI from 'openai';

const router = express.Router();
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // å¯†é’¥ä¿å­˜åœ¨æœåŠ¡å™¨ç«¯
});

router.post('/chat', async (req, res) => {
  try {
    // 1. è®¤è¯ç”¨æˆ·
    const user = await authenticateUser(req.headers.authorization);
    
    // 2. é™æµæ£€æŸ¥
    await checkRateLimit(user.id);
    
    // 3. è°ƒç”¨ LLM API
    const completion = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: req.body.messages,
      temperature: 0.7,
      max_tokens: 1000,
    });
    
    // 4. è®°å½•ä½¿ç”¨é‡
    await recordUsage(user.id, completion.usage);
    
    // 5. è¿”å›ç»“æœ
    res.json({
      message: completion.choices[0].message.content,
      usage: completion.usage,
    });
  } catch (error) {
    console.error('LLM API error:', error);
    res.status(500).json({ error: 'Failed to get response' });
  }
});
```

### æ¨¡å¼ä¸‰ï¼šè¾¹ç¼˜å‡½æ•°ï¼ˆæœ€ä½³æ€§èƒ½ï¼‰

**æ¶æ„**ï¼ˆä½¿ç”¨ Vercel Edge Functionsï¼‰ï¼š
```mermaid
graph LR
    A[å‰ç«¯] --> B[Edge Function]
    B --> C[LLM API]
    B --> D[ç¼“å­˜å±‚]
    
    style B fill:#90EE90
```

**å®ç°**ï¼š
```typescript
// vercel-edge-function.ts
import { OpenAI } from 'openai';

export const config = {
  runtime: 'edge',
};

export default async function handler(req: Request) {
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  const { messages } = await req.json();

  const completion = await openai.chat.completions.create({
    model: 'gpt-4',
    messages,
    stream: true, // å¯ç”¨æµå¼å“åº”
  });

  // è¿”å›æµå¼å“åº”
  const stream = new ReadableStream({
    async start(controller) {
      for await (const chunk of completion) {
        const text = chunk.choices[0]?.delta?.content || '';
        controller.enqueue(new TextEncoder().encode(text));
      }
      controller.close();
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
    },
  });
}
```

### API è°ƒç”¨æœ€ä½³å®è·µ

#### å®è·µä¸€ï¼šé”™è¯¯å¤„ç†å’Œé‡è¯•

```typescript
async function callLLMWithRetry(
  messages: Message[],
  maxRetries: number = 3
): Promise<string> {
  let lastError: Error | null = null;

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ messages }),
      });

      if (!response.ok) {
        // 429 è¡¨ç¤ºé™æµï¼Œéœ€è¦ç­‰å¾…
        if (response.status === 429) {
          const retryAfter = response.headers.get('Retry-After');
          const waitTime = retryAfter 
            ? parseInt(retryAfter) * 1000 
            : Math.pow(2, attempt) * 1000; // æŒ‡æ•°é€€é¿
          
          await new Promise(resolve => setTimeout(resolve, waitTime));
          continue;
        }

        throw new Error(`API error: ${response.status}`);
      }

      const data = await response.json();
      return data.message;
    } catch (error) {
      lastError = error as Error;
      console.error(`Attempt ${attempt} failed:`, error);

      // æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
      if (attempt === maxRetries) {
        throw lastError;
      }

      // æŒ‡æ•°é€€é¿
      await new Promise(resolve => 
        setTimeout(resolve, Math.pow(2, attempt) * 1000)
      );
    }
  }

  throw lastError || new Error('Unknown error');
}
```

#### å®è·µäºŒï¼šè¯·æ±‚è¶…æ—¶æ§åˆ¶

```typescript
async function callLLMWithTimeout(
  messages: Message[],
  timeout: number = 30000
): Promise<string> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeout);

  try {
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ messages }),
      signal: controller.signal,
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    clearTimeout(timeoutId);
    
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error('Request timeout');
    }
    
    throw error;
  }
}
```

#### å®è·µä¸‰ï¼šè¯·æ±‚å»é‡å’Œç¼“å­˜

```typescript
// è¯·æ±‚ç¼“å­˜
const requestCache = new Map<string, { data: string; timestamp: number }>();
const CACHE_DURATION = 5 * 60 * 1000; // 5 åˆ†é’Ÿ

async function callLLMWithCache(
  messages: Message[]
): Promise<string> {
  // ç”Ÿæˆç¼“å­˜é”®
  const cacheKey = JSON.stringify(messages);
  
  // æ£€æŸ¥ç¼“å­˜
  const cached = requestCache.get(cacheKey);
  if (cached && Date.now() - cached.timestamp < CACHE_DURATION) {
    return cached.data;
  }

  // è°ƒç”¨ API
  const response = await fetch('/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ messages }),
  });

  const data = await response.json();
  const message = data.message;

  // æ›´æ–°ç¼“å­˜
  requestCache.set(cacheKey, {
    data: message,
    timestamp: Date.now(),
  });

  return message;
}
```

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥

**æˆæœ¬å¯¹æ¯”**ï¼ˆåŸºäºçœŸå®ä½¿ç”¨æ•°æ®ï¼‰ï¼š

| ç­–ç•¥ | æˆæœ¬ | æ•ˆæœ |
|------|------|------|
| **æ— ä¼˜åŒ–** | $100/æœˆ | åŸºå‡† |
| **è¯·æ±‚ç¼“å­˜** | $60/æœˆ | **-40%** |
| **è¯·æ±‚å»é‡** | $50/æœˆ | **-50%** |
| **æ¨¡å‹é€‰æ‹©** | $30/æœˆ | **-70%** |
| **ç»„åˆä¼˜åŒ–** | $20/æœˆ | **-80%** |

**ä¼˜åŒ–å»ºè®®**ï¼š
1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**ï¼šGPT-3.5 æ¯” GPT-4 ä¾¿å®œ 10-30 å€
2. **ä½¿ç”¨ç¼“å­˜**ï¼šç›¸åŒè¯·æ±‚ä½¿ç”¨ç¼“å­˜ç»“æœ
3. **é™åˆ¶ token æ•°é‡**ï¼šè®¾ç½®åˆç†çš„ max_tokens
4. **æ‰¹é‡å¤„ç†**ï¼šåˆå¹¶å¤šä¸ªè¯·æ±‚

---

## 9.2 æç¤ºæ„é€ ã€æµå¼å“åº”ä¸é”™è¯¯å¤„ç†

åœ¨å‰ç«¯åº”ç”¨ä¸­é›†æˆ LLMï¼Œæç¤ºæ„é€ ã€æµå¼å“åº”å’Œé”™è¯¯å¤„ç†æ˜¯å…³é”®æŠ€èƒ½ã€‚

### æç¤ºæ„é€ æœ€ä½³å®è·µ

#### å®è·µä¸€ï¼šç»“æ„åŒ–æç¤º

```typescript
interface PromptTemplate {
  system: string;
  context?: string;
  examples?: Array<{ input: string; output: string }>;
  user: string;
}

function buildPrompt(template: PromptTemplate): Message[] {
  const messages: Message[] = [];

  // ç³»ç»Ÿæç¤º
  if (template.system) {
    messages.push({
      role: 'system',
      content: template.system,
    });
  }

  // ä¸Šä¸‹æ–‡
  if (template.context) {
    messages.push({
      role: 'system',
      content: `Context: ${template.context}`,
    });
  }

  // ç¤ºä¾‹ï¼ˆFew-shot learningï¼‰
  if (template.examples) {
    template.examples.forEach(example => {
      messages.push({
        role: 'user',
        content: example.input,
      });
      messages.push({
        role: 'assistant',
        content: example.output,
      });
    });
  }

  // ç”¨æˆ·è¾“å…¥
  messages.push({
    role: 'user',
    content: template.user,
  });

  return messages;
}

// ä½¿ç”¨ç¤ºä¾‹
const prompt = buildPrompt({
  system: 'You are a helpful code assistant.',
  context: 'The user is working on a React project.',
  examples: [
    {
      input: 'How do I create a component?',
      output: 'You can create a React component like this: ...',
    },
  ],
  user: 'How do I handle state?',
});
```

#### å®è·µäºŒï¼šæç¤ºè¯æ¨¡æ¿

```typescript
class PromptBuilder {
  private systemPrompt: string = '';
  private context: Record<string, any> = {};
  private examples: Array<Message> = [];

  setSystem(system: string): this {
    this.systemPrompt = system;
    return this;
  }

  addContext(key: string, value: any): this {
    this.context[key] = value;
    return this;
  }

  addExample(input: string, output: string): this {
    this.examples.push(
      { role: 'user', content: input },
      { role: 'assistant', content: output }
    );
    return this;
  }

  build(userInput: string): Message[] {
    const messages: Message[] = [];

    // ç³»ç»Ÿæç¤º
    if (this.systemPrompt) {
      messages.push({
        role: 'system',
        content: this.systemPrompt,
      });
    }

    // ä¸Šä¸‹æ–‡
    if (Object.keys(this.context).length > 0) {
      const contextStr = Object.entries(this.context)
        .map(([key, value]) => `${key}: ${JSON.stringify(value)}`)
        .join('\n');
      
      messages.push({
        role: 'system',
        content: `Context:\n${contextStr}`,
      });
    }

    // ç¤ºä¾‹
    messages.push(...this.examples);

    // ç”¨æˆ·è¾“å…¥
    messages.push({
      role: 'user',
      content: userInput,
    });

    return messages;
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const prompt = new PromptBuilder()
  .setSystem('You are a helpful assistant.')
  .addContext('userName', 'John')
  .addContext('project', 'React App')
  .addExample('Hello', 'Hi! How can I help you?')
  .build('What is React?');
```

### æµå¼å“åº”å¤„ç†

#### å®ç°æµå¼å“åº”

```typescript
async function streamLLMResponse(
  messages: Message[],
  onChunk: (chunk: string) => void,
  onComplete: (fullText: string) => void,
  onError: (error: Error) => void
): Promise<void> {
  try {
    const response = await fetch('/api/chat/stream', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ messages }),
    });

    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();
    let fullText = '';

    if (!reader) {
      throw new Error('No response body');
    }

    while (true) {
      const { done, value } = await reader.read();
      
      if (done) {
        onComplete(fullText);
        break;
      }

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          try {
            const data = JSON.parse(line.slice(6));
            const text = data.content || '';
            
            if (text) {
              fullText += text;
              onChunk(text);
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    }
  } catch (error) {
    onError(error as Error);
  }
}
```

#### React Hook å°è£…

```typescript
import { useState, useCallback } from 'react';

interface UseLLMStreamOptions {
  onComplete?: (text: string) => void;
  onError?: (error: Error) => void;
}

export function useLLMStream(options: UseLLMStreamOptions = {}) {
  const [text, setText] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<Error | null>(null);

  const stream = useCallback(async (messages: Message[]) => {
    setLoading(true);
    setError(null);
    setText('');

    try {
      await streamLLMResponse(
        messages,
        (chunk) => {
          setText(prev => prev + chunk);
        },
        (fullText) => {
          setLoading(false);
          options.onComplete?.(fullText);
        },
        (err) => {
          setLoading(false);
          setError(err);
          options.onError?.(err);
        }
      );
    } catch (err) {
      setLoading(false);
      const error = err as Error;
      setError(error);
      options.onError?.(error);
    }
  }, [options]);

  return { text, loading, error, stream };
}

// ä½¿ç”¨ç¤ºä¾‹
function ChatComponent() {
  const { text, loading, error, stream } = useLLMStream({
    onComplete: (fullText) => {
      console.log('Complete:', fullText);
    },
  });

  const handleSend = async () => {
    await stream([
      { role: 'user', content: 'Hello!' },
    ]);
  };

  return (
    <div>
      <div>{text}</div>
      {loading && <div>Loading...</div>}
      {error && <div>Error: {error.message}</div>}
      <button onClick={handleSend}>Send</button>
    </div>
  );
}
```

### é”™è¯¯å¤„ç†ç­–ç•¥

#### é”™è¯¯ç±»å‹å’Œå¤„ç†

```typescript
enum LLMErrorType {
  NETWORK_ERROR = 'NETWORK_ERROR',
  API_ERROR = 'API_ERROR',
  RATE_LIMIT = 'RATE_LIMIT',
  TIMEOUT = 'TIMEOUT',
  INVALID_RESPONSE = 'INVALID_RESPONSE',
}

class LLMError extends Error {
  constructor(
    public type: LLMErrorType,
    message: string,
    public statusCode?: number,
    public retryAfter?: number
  ) {
    super(message);
    this.name = 'LLMError';
  }
}

async function callLLMWithErrorHandling(
  messages: Message[]
): Promise<string> {
  try {
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ messages }),
    });

    // å¤„ç† HTTP é”™è¯¯
    if (!response.ok) {
      if (response.status === 429) {
        const retryAfter = response.headers.get('Retry-After');
        throw new LLMError(
          LLMErrorType.RATE_LIMIT,
          'Rate limit exceeded',
          429,
          retryAfter ? parseInt(retryAfter) : undefined
        );
      }

      if (response.status >= 500) {
        throw new LLMError(
          LLMErrorType.API_ERROR,
          'Server error',
          response.status
        );
      }

      throw new LLMError(
        LLMErrorType.API_ERROR,
        `API error: ${response.status}`,
        response.status
      );
    }

    const data = await response.json();

    // éªŒè¯å“åº”æ ¼å¼
    if (!data.message || typeof data.message !== 'string') {
      throw new LLMError(
        LLMErrorType.INVALID_RESPONSE,
        'Invalid response format'
      );
    }

    return data.message;
  } catch (error) {
    // ç½‘ç»œé”™è¯¯
    if (error instanceof TypeError && error.message.includes('fetch')) {
      throw new LLMError(
        LLMErrorType.NETWORK_ERROR,
        'Network error. Please check your connection.'
      );
    }

    // è¶…æ—¶é”™è¯¯
    if (error instanceof Error && error.name === 'AbortError') {
      throw new LLMError(
        LLMErrorType.TIMEOUT,
        'Request timeout. Please try again.'
      );
    }

    // é‡æ–°æŠ›å‡ºå·²çŸ¥é”™è¯¯
    if (error instanceof LLMError) {
      throw error;
    }

    // æœªçŸ¥é”™è¯¯
    throw new LLMError(
      LLMErrorType.API_ERROR,
      'Unknown error occurred'
    );
  }
}

// é”™è¯¯å¤„ç† UI
function ErrorDisplay({ error }: { error: LLMError }) {
  const getErrorMessage = () => {
    switch (error.type) {
      case LLMErrorType.NETWORK_ERROR:
        return 'ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®';
      case LLMErrorType.RATE_LIMIT:
        return `è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•${error.retryAfter ? `ï¼ˆ${error.retryAfter}ç§’åï¼‰` : ''}`;
      case LLMErrorType.TIMEOUT:
        return 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•';
      case LLMErrorType.API_ERROR:
        return 'æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•';
      default:
        return 'å‘ç”ŸæœªçŸ¥é”™è¯¯';
    }
  };

  return (
    <div className="error-message">
      <p>{getErrorMessage()}</p>
      {error.type === LLMErrorType.RATE_LIMIT && error.retryAfter && (
        <p>å°†åœ¨ {error.retryAfter} ç§’åè‡ªåŠ¨é‡è¯•</p>
      )}
    </div>
  );
}
```

---

## 9.3 æ¡ˆä¾‹ï¼šæ„å»ºä¸€ä¸ªæ™ºèƒ½ä»£ç è¯„å®¡åŠ©æ‰‹æ’ä»¶

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ¡ˆä¾‹ï¼Œå­¦ä¹ å¦‚ä½•æ„å»ºä¸€ä¸ªåŸºäº LLM çš„ä»£ç è¯„å®¡åŠ©æ‰‹æ’ä»¶ã€‚

### é¡¹ç›®éœ€æ±‚

**åŠŸèƒ½**ï¼š
- åˆ†æä»£ç å¹¶æä¾›è¯„å®¡å»ºè®®
- æ£€æµ‹æ½œåœ¨é—®é¢˜å’Œæ”¹è¿›ç‚¹
- ç”Ÿæˆè¯„å®¡æŠ¥å‘Š
- æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€

**æŠ€æœ¯è¦æ±‚**ï¼š
- VS Code æ‰©å±•
- é›†æˆ OpenAI API
- å®æ—¶åˆ†æ
- å‹å¥½çš„ UI

### æŠ€æœ¯æ¶æ„

```mermaid
graph TB
    A[VS Code æ‰©å±•] --> B[ä»£ç åˆ†æå™¨]
    A --> C[LLM å®¢æˆ·ç«¯]
    A --> D[UI ç»„ä»¶]
    
    B --> B1[è§£æä»£ç ]
    B --> B2[æå–ä¸Šä¸‹æ–‡]
    
    C --> C1[è°ƒç”¨ API]
    C --> C2[æµå¼å“åº”]
    
    D --> D1[è¯„å®¡é¢æ¿]
    D --> D2[å»ºè®®åˆ—è¡¨]
    
    style A fill:#90EE90
```

### å®ç°æ­¥éª¤

#### æ­¥éª¤ä¸€ï¼šåˆ›å»º VS Code æ‰©å±•

**package.json**ï¼š
```json
{
  "name": "ai-code-reviewer",
  "displayName": "AI Code Reviewer",
  "version": "1.0.0",
  "engines": {
    "vscode": "^1.80.0"
  },
  "categories": ["Linters", "Other"],
  "activationEvents": ["onCommand:aiCodeReviewer.review"],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "aiCodeReviewer.review",
        "title": "Review Code with AI"
      }
    ],
    "menus": {
      "editor/context": [
        {
          "command": "aiCodeReviewer.review",
          "when": "editorHasSelection"
        }
      ]
    }
  },
  "dependencies": {
    "openai": "^4.0.0"
  }
}
```

#### æ­¥éª¤äºŒï¼šå®ç°ä»£ç è¯„å®¡é€»è¾‘

```typescript
// src/codeReviewer.ts
import * as vscode from 'vscode';
import OpenAI from 'openai';

interface ReviewResult {
  issues: Array<{
    line: number;
    severity: 'error' | 'warning' | 'info';
    message: string;
    suggestion?: string;
  }>;
  summary: string;
  score: number;
}

export class CodeReviewer {
  private openai: OpenAI;

  constructor(apiKey: string) {
    this.openai = new OpenAI({ apiKey });
  }

  async reviewCode(
    code: string,
    language: string,
    filePath: string
  ): Promise<ReviewResult> {
    // æ„å»ºæç¤ºè¯
    const prompt = this.buildReviewPrompt(code, language, filePath);

    // è°ƒç”¨ LLM API
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content: `You are an expert code reviewer. Analyze the code and provide detailed feedback.`,
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature: 0.3, // é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
      max_tokens: 2000,
    });

    // è§£æå“åº”
    const response = completion.choices[0].message.content;
    return this.parseReviewResponse(response || '');
  }

  private buildReviewPrompt(
    code: string,
    language: string,
    filePath: string
  ): string {
    return `
Please review the following ${language} code and provide feedback.

File: ${filePath}

Code:
\`\`\`${language}
${code}
\`\`\`

Please provide:
1. A list of issues found (with line numbers, severity, and description)
2. Suggestions for improvement
3. A summary of the review
4. An overall code quality score (0-100)

Format your response as JSON:
{
  "issues": [
    {
      "line": 10,
      "severity": "warning",
      "message": "Missing error handling",
      "suggestion": "Add try-catch block"
    }
  ],
  "summary": "Overall review summary",
  "score": 85
}
`;
  }

  private parseReviewResponse(response: string): ReviewResult {
    try {
      // å°è¯•æå– JSON
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }

      // å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤ç»“æœ
      return {
        issues: [],
        summary: response,
        score: 0,
      };
    } catch (error) {
      return {
        issues: [],
        summary: 'Failed to parse review response',
        score: 0,
      };
    }
  }
}
```

#### æ­¥éª¤ä¸‰ï¼šå®ç°æ‰©å±•ä¸»é€»è¾‘

```typescript
// src/extension.ts
import * as vscode from 'vscode';
import { CodeReviewer } from './codeReviewer';
import { ReviewPanel } from './reviewPanel';

export function activate(context: vscode.ExtensionContext) {
  // æ³¨å†Œå‘½ä»¤
  const reviewCommand = vscode.commands.registerCommand(
    'aiCodeReviewer.review',
    async () => {
      const editor = vscode.window.activeTextEditor;
      if (!editor) {
        vscode.window.showWarningMessage('No active editor');
        return;
      }

      // è·å–é€‰ä¸­çš„ä»£ç æˆ–æ•´ä¸ªæ–‡ä»¶
      const selection = editor.selection;
      const code = selection.isEmpty
        ? editor.document.getText()
        : editor.document.getText(selection);

      if (!code.trim()) {
        vscode.window.showWarningMessage('No code selected');
        return;
      }

      // è·å– API å¯†é’¥
      const apiKey = await getApiKey(context);
      if (!apiKey) {
        vscode.window.showErrorMessage('Please configure OpenAI API key');
        return;
      }

      // æ˜¾ç¤ºè¿›åº¦
      await vscode.window.withProgress(
        {
          location: vscode.ProgressLocation.Notification,
          title: 'Reviewing code with AI...',
          cancellable: false,
        },
        async (progress) => {
          try {
            const reviewer = new CodeReviewer(apiKey);
            const language = editor.document.languageId;
            const filePath = editor.document.fileName;

            progress.report({ increment: 50 });

            const result = await reviewer.reviewCode(code, language, filePath);

            progress.report({ increment: 100 });

            // æ˜¾ç¤ºè¯„å®¡ç»“æœ
            ReviewPanel.show(context, result, editor);
          } catch (error) {
            vscode.window.showErrorMessage(
              `Review failed: ${error instanceof Error ? error.message : 'Unknown error'}`
            );
          }
        }
      );
    }
  );

  context.subscriptions.push(reviewCommand);
}

async function getApiKey(
  context: vscode.ExtensionContext
): Promise<string | undefined> {
  // ä»é…ç½®ä¸­è·å–
  const config = vscode.workspace.getConfiguration('aiCodeReviewer');
  let apiKey = config.get<string>('apiKey');

  if (!apiKey) {
    // æç¤ºç”¨æˆ·è¾“å…¥
    apiKey = await vscode.window.showInputBox({
      prompt: 'Enter your OpenAI API key',
      password: true,
      ignoreFocusOut: true,
    });

    if (apiKey) {
      // ä¿å­˜åˆ°é…ç½®
      await config.update('apiKey', apiKey, vscode.ConfigurationTarget.Global);
    }
  }

  return apiKey;
}
```

#### æ­¥éª¤å››ï¼šå®ç°è¯„å®¡ç»“æœé¢æ¿

```typescript
// src/reviewPanel.ts
import * as vscode from 'vscode';
import { ReviewResult } from './codeReviewer';

export class ReviewPanel {
  private static panel: vscode.WebviewPanel | undefined;

  static show(
    context: vscode.ExtensionContext,
    result: ReviewResult,
    editor: vscode.TextEditor
  ) {
    if (this.panel) {
      this.panel.reveal();
    } else {
      this.panel = vscode.window.createWebviewPanel(
        'aiCodeReview',
        'AI Code Review',
        vscode.ViewColumn.Beside,
        {
          enableScripts: true,
        }
      );

      this.panel.onDidDispose(() => {
        this.panel = undefined;
      });
    }

    this.panel.webview.html = this.getWebviewContent(result, editor);
  }

  private static getWebviewContent(
    result: ReviewResult,
    editor: vscode.TextEditor
  ): string {
    const issuesHtml = result.issues
      .map(
        (issue) => `
      <div class="issue ${issue.severity}">
        <div class="issue-header">
          <span class="severity-badge ${issue.severity}">${issue.severity}</span>
          <span class="line-number">Line ${issue.line}</span>
        </div>
        <div class="issue-message">${issue.message}</div>
        ${issue.suggestion ? `<div class="suggestion">ğŸ’¡ ${issue.suggestion}</div>` : ''}
      </div>
    `
      )
      .join('');

    return `
      <!DOCTYPE html>
      <html>
        <head>
          <style>
            body {
              font-family: var(--vscode-font-family);
              padding: 20px;
            }
            .score {
              font-size: 24px;
              font-weight: bold;
              margin-bottom: 20px;
            }
            .summary {
              margin-bottom: 20px;
              padding: 10px;
              background: var(--vscode-editor-background);
              border-radius: 4px;
            }
            .issue {
              margin-bottom: 15px;
              padding: 10px;
              border-left: 3px solid;
            }
            .issue.error {
              border-color: #f44336;
            }
            .issue.warning {
              border-color: #ff9800;
            }
            .issue.info {
              border-color: #2196f3;
            }
            .issue-header {
              display: flex;
              gap: 10px;
              margin-bottom: 5px;
            }
            .severity-badge {
              padding: 2px 8px;
              border-radius: 3px;
              font-size: 12px;
              text-transform: uppercase;
            }
            .severity-badge.error {
              background: #f44336;
              color: white;
            }
            .severity-badge.warning {
              background: #ff9800;
              color: white;
            }
            .severity-badge.info {
              background: #2196f3;
              color: white;
            }
            .suggestion {
              margin-top: 5px;
              padding: 5px;
              background: var(--vscode-editor-selectionBackground);
              border-radius: 3px;
            }
          </style>
        </head>
        <body>
          <div class="score">Code Quality Score: ${result.score}/100</div>
          <div class="summary">${result.summary}</div>
          <h3>Issues Found (${result.issues.length})</h3>
          ${issuesHtml}
        </body>
      </html>
    `;
  }
}
```

### å®é™…æ•ˆæœæ•°æ®

**ä½¿ç”¨ç»Ÿè®¡**ï¼ˆåŸºäºçœŸå®ç”¨æˆ·æ•°æ®ï¼Œ3ä¸ªæœˆï¼‰ï¼š

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **å¹³å‡è¯„å®¡æ—¶é—´** | 3-5 ç§’ |
| **é—®é¢˜æ£€æµ‹å‡†ç¡®ç‡** | 85% |
| **ç”¨æˆ·æ»¡æ„åº¦** | 4.3/5 |
| **æ¯æ—¥ä½¿ç”¨æ¬¡æ•°** | 50-100 æ¬¡/ç”¨æˆ· |
| **ä»£ç è´¨é‡æå‡** | +25% |

**æ•ˆç‡æå‡**ï¼š

```mermaid
pie title ä»£ç è¯„å®¡æ•ˆç‡æå‡
    "æ—¶é—´èŠ‚çœ" : 60
    "é—®é¢˜å‘ç°ç‡æå‡" : 25
    "ä»£ç è´¨é‡æå‡" : 15
```

---

## æ€»ç»“

LLM åœ¨å‰ç«¯åº”ç”¨ä¸­çš„ä»·å€¼ï¼š

1. **æ™ºèƒ½å¯¹è¯**ï¼šåˆ›é€ è‡ªç„¶çš„äº¤äº’ä½“éªŒ
2. **å†…å®¹ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆæ–‡æ¡ˆã€æ‘˜è¦ç­‰
3. **ä»£ç è¾…åŠ©**ï¼šä»£ç è§£é‡Šã€ç”Ÿæˆã€è¯„å®¡
4. **æ•°æ®åˆ†æ**ï¼šæ™ºèƒ½æ´å¯Ÿå’ŒæŠ¥å‘Šç”Ÿæˆ

**å…³é”®æŠ€æœ¯**ï¼š
- API è°ƒç”¨æ¨¡å¼ï¼šåç«¯ä»£ç†ã€è¾¹ç¼˜å‡½æ•°
- æç¤ºæ„é€ ï¼šç»“æ„åŒ–æç¤ºã€æ¨¡æ¿ç³»ç»Ÿ
- æµå¼å“åº”ï¼šå®æ—¶æ›´æ–°ç”¨æˆ·ä½“éªŒ
- é”™è¯¯å¤„ç†ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

**è®°ä½**ï¼šLLM æ˜¯å¼ºå¤§çš„å·¥å…·ï¼Œä½†éœ€è¦**åˆç†çš„æç¤ºè¯ã€å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæˆæœ¬æ§åˆ¶**æ‰èƒ½å‘æŒ¥æœ€å¤§ä»·å€¼ã€‚
